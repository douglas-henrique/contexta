# ğŸ§ª Complete Testing Guide - Contexta

## ğŸ“Š Overview

Complete unit test suite to ensure code quality and reliability.

### Statistics
- **Total Test Files**: 12
- **Modules Covered**: Core, Ingest, API
- **Test Types**: Unit, Integration (CI/CD ready)
- **Framework**: pytest + pytest-cov + pytest-mock

---

## ğŸ—‚ï¸ Test Structure

```
tests/
â”œâ”€â”€ conftest.py                    # ğŸ”§ Shared fixtures
â”‚   â”œâ”€â”€ mock_openai_client         # OpenAI mock
â”‚   â”œâ”€â”€ mock_qdrant_client         # Qdrant mock
â”‚   â”œâ”€â”€ sample_text                # Sample texts
â”‚   â”œâ”€â”€ sample_chunks               # Sample chunks
â”‚   â”œâ”€â”€ sample_embeddings          # Sample embeddings
â”‚   â””â”€â”€ sample_search_results      # Sample search results
â”‚
â”œâ”€â”€ test_core/                     # âœ… Core Tests (Framework-agnostic)
â”‚   â”œâ”€â”€ test_llm.py                # LLM provider tests
â”‚   â”‚   â””â”€â”€ TestOpenAILLM
â”‚   â”‚       â”œâ”€â”€ test_initialization
â”‚   â”‚       â”œâ”€â”€ test_generate
â”‚   â”‚       â”œâ”€â”€ test_generate_with_parameters
â”‚   â”‚       â””â”€â”€ test_generate_stream
â”‚   â”‚   
â”‚   â”œâ”€â”€ test_prompts.py            # Prompt Builder tests
â”‚   â”‚   â””â”€â”€ TestRAGPromptBuilder
â”‚   â”‚       â”œâ”€â”€ test_initialization
â”‚   â”‚       â”œâ”€â”€ test_build_basic
â”‚   â”‚       â”œâ”€â”€ test_build_with_max_length
â”‚   â”‚       â””â”€â”€ test_build_with_sources
â”‚   â”‚
â”‚   â””â”€â”€ test_reranker.py           # Re-ranker tests
â”‚       â””â”€â”€ TestSimpleReranker
â”‚           â”œâ”€â”€ test_rerank_by_score
â”‚           â”œâ”€â”€ test_rerank_top_k
â”‚           â””â”€â”€ test_rerank_empty_results
â”‚
â”œâ”€â”€ test_ingest/                   # ğŸ“¥ Ingest Service Tests
â”‚   â”œâ”€â”€ test_chunking.py           # Chunking tests
â”‚   â”‚   â””â”€â”€ TestSemanticChunking
â”‚   â”‚       â”œâ”€â”€ test_basic_chunking
â”‚   â”‚       â”œâ”€â”€ test_chunk_size
â”‚   â”‚       â”œâ”€â”€ test_chunk_overlap
â”‚   â”‚       â””â”€â”€ test_empty_text
â”‚   â”‚
â”‚   â”œâ”€â”€ test_loaders.py            # Document Loader tests
â”‚   â”‚   â”œâ”€â”€ TestPDFLoader
â”‚   â”‚   â”‚   â””â”€â”€ test_load_pdf
â”‚   â”‚   â””â”€â”€ TestLoaderFactory
â”‚   â”‚       â”œâ”€â”€ test_get_loader_pdf
â”‚   â”‚       â”œâ”€â”€ test_load_document_pdf
â”‚   â”‚       â””â”€â”€ test_load_document_txt
â”‚   â”‚
â”‚   â”œâ”€â”€ test_embeddings.py         # Embedding tests
â”‚   â”‚   â””â”€â”€ TestOpenAIEmbeddings
â”‚   â”‚       â”œâ”€â”€ test_embed_texts
â”‚   â”‚       â””â”€â”€ test_embed_texts_custom_model
â”‚   â”‚
â”‚   â””â”€â”€ test_vectorstore.py        # Vector Store tests
â”‚       â””â”€â”€ TestQdrantVectorStore
â”‚           â”œâ”€â”€ test_ensure_collection_exists
â”‚           â”œâ”€â”€ test_store_embeddings
â”‚           â”œâ”€â”€ test_search
â”‚           â””â”€â”€ test_search_with_filters
â”‚
â””â”€â”€ test_api/                      # ğŸš€ API Tests
    â””â”€â”€ test_main.py               # Endpoint tests
        â””â”€â”€ TestAPIEndpoints
            â”œâ”€â”€ test_root_endpoint
            â”œâ”€â”€ test_health_endpoint
            â”œâ”€â”€ test_query_endpoint
            â”œâ”€â”€ test_query_endpoint_no_results
            â””â”€â”€ test_query_endpoint_validation
```

---

## ğŸš€ How to Run

### Method 1: Test Script (Recommended)

```bash
# All tests
./run_tests.sh

# Tests with coverage (generates HTML report)
./run_tests.sh cov

# Only unit tests
./run_tests.sh unit

# Only integration tests
./run_tests.sh integration

# Fast tests (excludes @pytest.mark.slow)
./run_tests.sh fast

# Watch mode (reruns on file save)
./run_tests.sh watch
```

### Method 2: Poetry Direct

```bash
# All tests with verbosity
poetry run pytest -v

# With coverage
poetry run pytest --cov --cov-report=term-missing

# Specific tests
poetry run pytest tests/test_core/test_llm.py

# A specific class
poetry run pytest tests/test_core/test_llm.py::TestOpenAILLM

# A specific test
poetry run pytest tests/test_core/test_llm.py::TestOpenAILLM::test_generate

# Stop on first error
poetry run pytest -x

# Show outputs (print statements)
poetry run pytest -s

# Watch mode
poetry run pytest-watch
```

### Method 3: Makefile

```bash
# All tests
make test

# With coverage
make test-cov

# Only unit tests
make test-unit

# Only integration tests
make test-integration
```

### Method 4: Docker

```bash
# Run tests in container
docker-compose run --rm ingest poetry run pytest

# With coverage
docker-compose run --rm ingest poetry run pytest --cov

# Using make
make docker-test
```

---

## ğŸ“ˆ Code Coverage

### Generate Report

```bash
# Terminal
poetry run pytest --cov --cov-report=term-missing

# HTML (opens in browser)
poetry run pytest --cov --cov-report=html
open htmlcov/index.html

# XML (for CI/CD)
poetry run pytest --cov --cov-report=xml
```

### Coverage Goals

- **Core**: 90%+
- **Ingest**: 85%+
- **API**: 80%+
- **Overall**: 85%+

---

## ğŸ¯ Test Types

### Unit Tests (`@pytest.mark.unit`)

- Test isolated components
- Use mocks for external dependencies
- Fast to run (<1s per test)
- Don't require external services

**Example:**
```python
@pytest.mark.unit
def test_generate(mock_openai_client):
    llm = OpenAILLM(api_key="test")
    result = llm.generate("Test prompt")
    assert result == "Generated text"
```

### Integration Tests (`@pytest.mark.integration`)

- Test interaction between components
- May use real services (Qdrant, etc.)
- Slower
- Ideal for CI/CD pipeline

**Example:**
```python
@pytest.mark.integration
def test_end_to_end_query():
    # Uses real Qdrant, real OpenAI
    response = client.post("/query", json={"query": "test"})
    assert response.status_code == 200
```

### Slow Tests (`@pytest.mark.slow`)

- Tests that take >5s
- Usually integration or end-to-end tests
- Can be skipped during rapid development

```bash
# Skip slow tests
poetry run pytest -m "not slow"
```

---

## ğŸ”§ Available Fixtures

### Mock Clients

```python
def test_with_openai(mock_openai_client):
    """mock_openai_client already configured with fake responses"""
    pass

def test_with_qdrant(mock_qdrant_client):
    """mock_qdrant_client already configured with fake results"""
    pass
```

### Sample Data

```python
def test_chunking(sample_text):
    """sample_text contains sample text"""
    chunks = semantic_chunk(sample_text)
    assert len(chunks) > 0

def test_embeddings(sample_chunks):
    """sample_chunks contains list of chunks"""
    pass

def test_search(sample_search_results):
    """sample_search_results contains mock search results"""
    pass
```

---

## ğŸ§© Best Practices Implemented

### 1. Arrange-Act-Assert (AAA)

```python
def test_example():
    # Arrange: setup
    llm = OpenAILLM(api_key="test")
    
    # Act: execute
    result = llm.generate("prompt")
    
    # Assert: verify
    assert result == "expected"
```

### 2. Descriptive Test Names

```python
# âœ… Good
def test_generate_with_invalid_api_key_raises_error():
    pass

# âŒ Bad
def test_generate_error():
    pass
```

### 3. One Assert Per Test (when possible)

```python
# âœ… Good
def test_response_has_answer():
    assert "answer" in response.json()

def test_response_has_sources():
    assert "sources" in response.json()

# âŒ Avoid multiple unrelated asserts
def test_response():
    assert "answer" in response.json()
    assert "sources" in response.json()
    assert response.status_code == 200
```

### 4. Use Mocks for External APIs

```python
# âœ… Always use mocks for OpenAI, Qdrant in unit tests
@patch('ingest.embeddings.openai.OpenAI')
def test_embed_texts(mock_openai):
    # Fast test, no API cost
    pass

# âŒ Never make real calls in unit tests
def test_embed_texts_real():
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    # This is slow and costs money!
```

### 5. Organize in Classes

```python
class TestOpenAILLM:
    """Groups all tests related to OpenAILLM"""
    
    def test_initialization(self):
        pass
    
    def test_generate(self):
        pass
    
    def test_generate_stream(self):
        pass
```

---

## ğŸš¨ Troubleshooting

### Import Errors

```bash
# Add to PYTHONPATH
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
poetry run pytest
```

### Tests Failing Locally but Passing in CI

- Check environment variables (`.env` vs CI secrets)
- Check dependencies (different versions?)
- Check operating system (paths, line endings)

### Mock Not Working

```python
# âŒ Bad: mock in wrong place
@patch('core.llm.openai.OpenAI')  # Original import

# âœ… Good: mock where it's used
@patch('tests.test_core.test_llm.OpenAI')  # Where test imports
```

### Tests Too Slow

- Use `-n auto` for parallelization: `pytest -n auto`
- Skip slow tests: `pytest -m "not slow"`
- Check if you're using mocks correctly

---

## ğŸ”„ CI/CD

### GitHub Actions

Tests run automatically on each push/PR.

**Workflow**: `.github/workflows/tests.yml`

- âœ… Runs on Python 3.12
- âœ… Starts Qdrant as service
- âœ… Runs all tests
- âœ… Generates coverage report
- âœ… Upload to Codecov (optional)
- âœ… Linting (flake8, black, isort)

### Badges (add to README.md)

```markdown
![Tests](https://github.com/your-user/contexta/workflows/Tests/badge.svg)
[![codecov](https://codecov.io/gh/your-user/contexta/branch/main/graph/badge.svg)](https://codecov.io/gh/your-user/contexta)
```

---

## ğŸ“š Additional Resources

### Pytest Documentation

- [pytest docs](https://docs.pytest.org/)
- [pytest-cov](https://pytest-cov.readthedocs.io/)
- [pytest-mock](https://pytest-mock.readthedocs.io/)

### Next Steps

- [ ] Add complete end-to-end tests
- [ ] Add performance tests
- [ ] Add load tests (locust, k6)
- [ ] Configure Codecov for coverage reports
- [ ] Add mutation testing (mutmut)
- [ ] Add property-based testing (hypothesis)

---

## âœ… Testing Checklist

Before making a PR, ensure:

- [ ] All tests pass: `./run_tests.sh`
- [ ] Coverage >85%: `./run_tests.sh cov`
- [ ] Linting ok: `make lint`
- [ ] Formatting ok: `make format`
- [ ] New tests added for new features
- [ ] Tests use appropriate mocks
- [ ] Tests have descriptive names
- [ ] Documentation updated if necessary

---

**ğŸ‰ Ready! You now have a professional test suite for Contexta!**
